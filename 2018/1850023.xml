<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Bayesian Models for Fairness, and Fairness for Bayesian Models</AwardTitle>
<AwardEffectiveDate>07/01/2019</AwardEffectiveDate>
<AwardExpirationDate>06/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>174869.00</AwardTotalIntnAmount>
<AwardAmount>174869</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Weng-keen Wong</SignBlockName>
</ProgramOfficer>
<AbstractNarration>In our interconnected society, artificial intelligence (AI) and machine learning (ML) systems have become ubiquitous.  Every day, machine learning systems influence our purchasing decisions, our navigation through virtual and physical spaces, the friendships we make, and even the romantic relationships we form.  The decisions automated by these systems have increasingly important real-world consequences, from credit scoring, to college admissions, to the prediction of re-offending behavior in the criminal justice system, as is already being used for bail and sentencing decisions across the United States of America.  With the growing impact of artificial intelligence and machine learning technologies on our society, and their importance to the economic competitiveness and technological leadership of the United States, it is imperative that we ensure that these systems behave in a fair and trustworthy manner.  Recent studies have shown that data-driven AI and ML systems can in some cases exhibit unfair and unjust behavior, for example due to biases hidden in the input data, or because of flawed engineering decisions.  This project develops a suite of tools for modeling, measuring, and correcting unfair and discriminatory behavior in AI and ML systems.  The research focuses on simultaneously addressing algorithmic discrimination that may occur across several overlapping dimensions, including gender, race, national origin, sexual orientation, disability status, and socioeconomic class.  The novel AI techniques developed in this project address the two main technical challenges which specifically arise in this context: uncertainty in the measurement of fairness, and correlations in the data.&lt;br/&gt;&lt;br/&gt;When ensuring AI fairness regarding multiple protected dimensions such as gender and race, data sparsity rapidly becomes a challenge as the number of dimensions, or the number of values per dimension, increase.  This data sparsity directly results in uncertainty in the measurement of fairness.  The project will leverage Bayesian inference, a branch of statistics which specifically addresses uncertainty, to manage this issue.  Correlations between the protected (and other) attributes will be leveraged using probabilistic graphical models, a class of machine learning models which encode dependence relationships.   Using a novel Bayesian definition of fairness as a unifying framework, the project's contributions consist of three interdependent tracks.  The first track will focus on developing general modeling techniques for the statistically efficient measurement of fairness, using latent variable models to produce parsimonious representations, and hierarchical modeling to achieve data efficiency.  The second track develops adversarial optimization algorithms to train machine learning algorithms to respect fairness constraints when the data distribution is uncertain.  In the third track, the project will develop methods for ensuring fairness in Bayesian inference, which can be used to prevent the inferences from reflecting negative stereotypes. The methods will be validated with case studies on applications across a wide range of data regimes, including modeling census income data, criminal justice recidivism prediction, and social media analytics.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/14/2019</MinAmdLetterDate>
<MaxAmdLetterDate>03/14/2019</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1850023</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Foulds</LastName>
<EmailAddress>jfoulds@umbc.edu</EmailAddress>
<StartDate>03/14/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland Baltimore County</Name>
<CityName>Baltimore</CityName>
<ZipCode>212500002</ZipCode>
<PhoneNumber>4104553140</PhoneNumber>
<StreetAddress>1000 Hilltop Circle</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
</Institution>
<ProgramElement>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
</Award>
</rootTag>
