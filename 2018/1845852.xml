<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Information-Theoretic Foundations of Fairness in Machine Learning</AwardTitle>
<AwardEffectiveDate>02/01/2019</AwardEffectiveDate>
<AwardExpirationDate>01/31/2024</AwardExpirationDate>
<AwardAmount>106915</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Machine learning algorithms can identify complex patterns in very large datasets. These algorithms are increasingly used in applications of significant social consequence, such as loan approval, hiring, and bail and sentencing decisions. However, real-world data may reflect discrimination patterns that exist in society at large. Consequently, decisions based on algorithms that learn from data are at risk of inheriting and, ultimately, reinforcing discriminatory and unfair social biases. This project aims to precisely characterize the operational limits of discrimination discovery and control in machine learning by combining legal and social science definitions of fairness with powerful mathematical tools from information theory, statistics, and optimization. This cross-disciplinary effort aims to provide fundamental theory and design guidelines for data scientists and engineers who will create the next generation of fair data-driven algorithms and applications. The technical results of this project will also inform the debate surrounding the social impact of machine learning. Moreover, this research will be used as a vessel for engaging students and researchers from diverse backgrounds in the applicability of information theory, machine learning, optimization, and, more broadly, math and engineering to social challenges.&lt;br/&gt;&lt;br/&gt;Automated methods for discovering and controlling discrimination in machine learning inherently face a trade-off between fairness and accuracy, and are limited by the dimensionality of the underlying data. This project creates a comprehensive information-theoretic framework that captures the limits of discrimination control by determining (i) how to systematically identify data features that may lead to discrimination; (ii) how to ensure fairness by producing new, information-theoretically grounded data representations; (iii) the fundamental information-theoretic trade-offs between fairness, distortion, and accuracy; and (iv) the impact of finite samples in discrimination detection and mitigation. The key advantage of the information-theoretic methodology adopted in this project is that it captures fundamental, algorithm-independent properties of discrimination, while being fertile ground for the development of novel mathematical tools and models relevant to both data scientists and information theorists. The theoretical component of this research weaves new connections between information theory and robust statistics by analyzing the impact of local perturbations of probability distributions on discrimination metrics, and creates new information-theoretic models useful in discrimination control, privacy, and representation learning. The applied component of this research develops robust, data-driven methods for measuring and mitigating discrimination that are immediately relevant for fair algorithmic decision-making in applications of consequence.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>12/07/2018</MinAmdLetterDate>
<MaxAmdLetterDate>12/07/2018</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1845852</AwardID>
<Investigator>
<FirstName>Flavio</FirstName>
<LastName>Calmon</LastName>
<EmailAddress>flavio@seas.harvard.edu</EmailAddress>
<StartDate>12/07/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
</Institution>
<ProgramElement>
<Code>7797</Code>
<Text>COMM &amp; INFORMATION FOUNDATIONS</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
</Award>
</rootTag>
