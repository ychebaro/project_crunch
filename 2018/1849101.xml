<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Pilot Study on Bias and Trust in AI Systems</AwardTitle>
<AwardEffectiveDate>10/01/2018</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardAmount>74529</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tonya Smith-Jackson</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Robots and other autonomous systems are proliferating rapidly, and are being used in the public domain for such purposes as companionship, facilitating interactive learning, and making recommendations to users. However, little is understood about how trust, including mistrust and over-trust, and bias develop as humans interact with these systems. The investigators of this project will use basic real-life scenarios to understand human-human interaction in the context of trust and bias, and then will translate these data into quantized attributes.  This data will be used to develop general algorithms that will be useful in programming robots in ways that ensure human safety and well-being when interacting with robots. This project will conduct a series of pilot tests to explore fundamental research questions to find ways to minimize potential negative impacts and consequences of human-robot interaction. Additionally, the investigators will integrate the research with teaching and training of graduate and undergraduate students.&lt;br/&gt;&lt;br/&gt;The aim of this project is to gain knowledge that informs the design of future robot systems by understanding how trust is established and how bias impacts human perception and algorithmic performance. As part of this work, the team also aims to establish baseline algorithms that mitigate the impacts of bias in autonomous systems. The specific research objectives of this project are to quantify the impact of trust and bias in human-robot interaction scenarios where the algorithms are designed based on learned data from human experts; and to develop methods for objectively mitigating bias, while still optimizing for robot performance.  The pilot tests are designed to contribute to understanding of social cognition in human-robot interaction. Direct societal befits will be gained as robots and other autonomous systems proliferate in the public domain.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/15/2018</MinAmdLetterDate>
<MaxAmdLetterDate>09/15/2018</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1849101</AwardID>
<Investigator>
<FirstName>Jason</FirstName>
<LastName>Borenstein</LastName>
<EmailAddress>jason.borenstein@pubpolicy.gatech.edu</EmailAddress>
<StartDate>09/15/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ayanna</FirstName>
<LastName>Howard</LastName>
<EmailAddress>ah260@gatech.edu</EmailAddress>
<StartDate>09/15/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
</Institution>
<ProgramElement>
<Code>7367</Code>
<Text>Cyber-Human Systems (CHS)</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
</Award>
</rootTag>
