<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Inexact Optimization Methods for Structured Nonlinear Optimization</AwardTitle>
<AwardEffectiveDate>07/15/2018</AwardEffectiveDate>
<AwardExpirationDate>06/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>200000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Leland Jameson</SignBlockName>
</ProgramOfficer>
<AbstractNarration>New efficient computational algorithms will be developed for solving large-scale optimization problems with particular structure. Structured nonlinear optimization has played a central role in various modern applications ranging from image processing, optimal control to stochastic learning in big data area. The algorithms developed in the project will provide solutions in a more robust and faster way, and will be made publicly available to benefit both optimization and computational data science community. The student supported in this project will have excellent opportunities for interdisciplinary research.&lt;br/&gt;&lt;br/&gt;The current methods for solving structured optimization problems often need to solve a sequence of subproblems according to the problem structure. This project aims to develop efficient methods and software that allow to solve their subproblems inexactly while still theoretically guarantee the global convergence and maintain the same or almost the same computational complexity of the corresponding methods that require exact solve of the subproblems. In particular, the investigator will develop (I) a framework of inexact alternating direction methods of multipliers for separable convex optimization, where the subproblem is solved to the accuracy relative to the whole problem KKT error; (II) inexact stochastic gradient methods for the composite optimization, which combines the(accelerated) proximal gradient methods and stochastic variance reduction techniques; (III) inexact active-set algorithms for polyhedral constrained nonlinear optimization.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/12/2018</MinAmdLetterDate>
<MaxAmdLetterDate>07/12/2018</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1819161</AwardID>
</Award>
</rootTag>
