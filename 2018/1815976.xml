<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Enhancing Educational Virtual Reality with Headset-based Eye Tracking</AwardTitle>
<AwardEffectiveDate>08/01/2018</AwardEffectiveDate>
<AwardExpirationDate>07/31/2021</AwardExpirationDate>
<AwardAmount>499813</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Dan Cosley</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Virtual reality (VR) can bring lab or field-trip-like experiences to students who are unable to visit physical sites because of location, budget, or schedule.  Potential advantages of these experiences over traditional teaching tools include increased student engagement and motivation, more direct viewing of size and spatial relationships of modeled objects, and stronger memories of the content.  Emerging consumer VR devices are starting to provide sufficient quality and affordability for home and school use, and this will eventually make educational VR experiences broadly available.  Future consumer VR headsets are expected to include increased sensing, such as eye tracking cameras to determine where users are looking and strain gauges to detect facial expressions. The sensor data can be analyzed for insight into users' attention and emotional affect.  The project will investigate how such insight into student attention can be used to improve educational VR through the design of personalized educational environments that respond to individual students' attention. The project will also develop techniques for using sensor data to give teachers enhanced real-time insight into student activities and behavior patterns to help them provide better teacher-guided VR experiences.  This will involve development of new approaches for educational VR technology and experiments that generate fundamental knowledge and guidelines for applying such approaches.  In addition to the potential long-term benefit of improving education, the project will provide a number of more immediate, direct educational benefits.  The team will incorporate the work into courses and undergraduate research experiences on human-computer interaction and VR, as well as outreach activities and summer programs aimed at high school students across Louisiana. &lt;br/&gt;&lt;br/&gt;The team will design and assess methods including the following: 1) educational content that responds to student eye gaze for more responsive and engaging presentation; 2) visual effects or indicators, based on detected eye behaviors, to encourage student attention to particular content in a VR environment; and 3) visualizations of student eye gaze that use both raw and processed gaze data to help teachers understand and guide students. To understand the tradeoffs between approaches and to develop guidelines for wider development and use of these techniques, effects will be studied in terms of behavior, subjective experience, and learning. The most promising methods will be applied to a case study of a networked VR interface that allows teachers to monitor and guide students through an immersive educational VR environment. To do this, the team will build on their existing educational VR framework that has previously been deployed at regional high schools and to thousands of students at outreach events. The project is expected to improve the effectiveness of such VR systems and of teachers' ability to supervise and assist students.  Resulting methods and principles will provide a foundation for headset-based eye tracking in educational VR and in other related applications such as simulation-based training and accessibility.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/31/2018</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2018</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1815976</AwardID>
<Investigator>
<FirstName>Arun</FirstName>
<LastName>Kulshreshth</LastName>
<EmailAddress>arunkul@louisiana.edu</EmailAddress>
<StartDate>07/31/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christoph</FirstName>
<LastName>Borst</LastName>
<EmailAddress>cwborst@gmail.com</EmailAddress>
<StartDate>07/31/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Louisiana at Lafayette</Name>
<CityName>Lafayette</CityName>
<ZipCode>705032701</ZipCode>
<PhoneNumber>3374825811</PhoneNumber>
<StreetAddress>104 E University Ave</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Louisiana</StateName>
<StateCode>LA</StateCode>
</Institution>
<ProgramElement>
<Code>7367</Code>
<Text>Cyber-Human Systems (CHS)</Text>
</ProgramElement>
<ProgramElement>
<Code>8020</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8045</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramReference>
</Award>
</rootTag>
